#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""driving_gridworld.safety_stats.oct29_2018.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CrwYLO3eO8-k-JLQmqQj0O8HEFKb112y

# Setup

## Installs
"""

import os

try:
    import pycolab
except:
    os.system('rm -rf pycolab')
    os.system('git clone https://github.com/deepmind/pycolab.git')
    os.system('cd pycolab/ && pip install .')

git_checkpoint = 'git checkout master && git pull && git rev-list -n1 --before=2018-11-01 master | xargs git checkout'


def pip_install(lib='.'):
    return 'pip install {0} -q && pip install {0} --upgrade --no-deps -q'.format(
        lib)


pip_install_here = pip_install()

try:
    import research2018
    import robust_offline_contextual_bandits
    import driving_gridworld_experiments
except:
    os.system('rm -rf research2018')
    os.system('git clone https://github.com/dmorrill10/research2018.git')
    os.system('cd research2018 && $git_checkpoint')
    os.system('cd research2018/research2018 && {}'.format(pip_install_here))
    os.system('cd research2018/robust_offline_contextual_bandits && {}'.format(
        pip_install_here))
    os.system('cd research2018/driving_gridworld_experiments && {}'.format(
        pip_install_here))

try:
    import driving_gridworld
except:
    os.system('rm -r driving_gridworld')
    os.system('git clone https://github.com/AmiiThinks/driving_gridworld.git')
    os.system('cd driving_gridworld/ && $git_checkpoint && {}'.format(
        pip_install_here))

try:
    import tf_supervised_inference
except:
    os.system('rm -rf tf-supervised_inference')
    os.system(
        'git clone https://github.com/AmiiThinks/tf-supervised_inference.git')
    os.system('cd tf-supervised_inference/ && $git_checkpoint && {}'.format(
        pip_install_here))

try:
    import tf_contextual_prediction_with_expert_advice
except:
    os.system('rm -rf tf-contextual_prediction_with_expert_advice')
    os.system(
        'git clone https://github.com/AmiiThinks/tf-contextual_prediction_with_expert_advice.git'
    )
    os.system(
        'cd tf-contextual_prediction_with_expert_advice/ && $git_checkpoint && {}'.
        format(pip_install_here))

try:
    import tf_kofn_robust_policy_optimization
except:
    os.system('rm -rf tf-kofn_robust_policy_optimization')
    os.system(
        'git clone https://github.com/AmiiThinks/tf-kofn_robust_policy_optimization.git'
    )
    os.system(
        'cd tf-kofn_robust_policy_optimization/ && $git_checkpoint && {}'.
        format(pip_install_here))

try:
    from simple_pytimer import Timer, AccumulatingTimer
except:
    os.system('pip install simple-pytimer')
    from simple_pytimer import Timer, AccumulatingTimer
"""## Import and Definitions"""

import tensorflow as tf
import numpy as np

from driving_gridworld.gridworld import DrivingGridworld
from driving_gridworld.rewards import SituationalReward
from driving_gridworld.road import Road
from driving_gridworld.car import Car
from driving_gridworld.obstacles import Bump, Pedestrian

from tf_kofn_robust_policy_optimization.robust.kofn import \
    KofnEvsAndWeights, \
    kofn_action_values
from tf_kofn_robust_policy_optimization.robust import deterministic_kofn_weights
from tf_kofn_robust_policy_optimization.discounted_mdp import \
  generalized_policy_iteration_op, \
  dual_state_value_policy_evaluation_op

from robust_offline_contextual_bandits.plotting import set_good_defaults
from research2018.data import load_or_save, load_list
from robust_offline_contextual_bandits.tf_np import reset_random

from driving_gridworld_experiments import \
  UncertainRewardDiscountedContinuingKofnTabularCfr, \
  KofnCfrLearner

from robust_offline_contextual_bandits.plotting import tableu20_color_table, line_style_table
import matplotlib.pyplot as plt


def new_road(headlight_range=3):
    return Road(
        headlight_range,
        Car(2, 0),
        obstacles=[
            Bump(-1, -1, prob_of_appearing=0.16),
            Pedestrian(-1, -1, speed=1, prob_of_appearing=0.13)
        ],
        allowed_obstacle_appearance_columns=[{2}, {1}],
        allow_crashing=True)


"""## Global Settings"""

set_good_defaults()
np.set_printoptions(formatter={'float': '{:0.4f}'.format})

os.system('ls')
"""# Experiments

## Static Definitions
"""

headlight_range = 3
speed_limit = new_road(headlight_range=headlight_range).speed_limit()

game = DrivingGridworld(
    lambda: new_road(headlight_range=headlight_range),
    discount=1.0,
    reward_function=lambda *args, **kwargs: 0)

num_samples_per_cfr_iter = 10
n = 100
num_reward_functions = n * num_samples_per_cfr_iter
num_train_and_test_reward_functions = 2 * num_reward_functions

reward_function_dist_timer = Timer('reward distribution generation')
with reward_function_dist_timer:
    z = speed_limit * speed_limit + speed_limit + 1
    wc_ncer = tf.fill([num_train_and_test_reward_functions], -1.0 / z)

    random_reward_function = SituationalReward(
        stopping_reward=tf.zeros([num_train_and_test_reward_functions]),
        wc_non_critical_error_reward=wc_ncer,
        bc_unobstructed_progress_reward=-wc_ncer,
        num_samples=num_train_and_test_reward_functions,
        critical_error_reward=10 * z * wc_ncer)
    transitions_tensor, rfd_list, state_indices = game.road.tabulate(
        random_reward_function, print_every=100)
    transitions_tensor = tf.stack(transitions_tensor)
    num_states = transitions_tensor.shape[0]
    num_actions = transitions_tensor.shape[1]

    rfd_tensor = tf.stack(rfd_list)
    reward_dataset = rfd_tensor[:, :, :num_reward_functions]
    test_reward_dataset = rfd_tensor[:, :, num_reward_functions:]

    reward_dataset = tf.transpose(reward_dataset, [2, 0, 1])
    test_reward_dataset = tf.transpose(test_reward_dataset, [2, 0, 1])
print(reward_dataset.shape)
print(test_reward_dataset.shape)
print(reward_function_dist_timer)

num_states = transitions_tensor.shape[0]
num_actions = transitions_tensor.shape[1]
num_states

root_probs_tf = tf.one_hot(
    state_indices[game.road.copy().to_key()], depth=num_states)

sasp_safety_info, _si = game.road.safety_information()
for k, v in _si.items():
    assert state_indices[k] == v
sasp_safety_info = np.array(sasp_safety_info)

sa_safety_info = tf.transpose(
    tf.reduce_sum(
        sasp_safety_info * tf.expand_dims(transitions_tensor, axis=-1),
        axis=2), [2, 0, 1])
print(sa_safety_info.shape)
"""## k-of-n Policies

### Imports and Definitions
"""

_n = 40
ks = list(range(_n // 10, _n + 1, _n // 10))
# ks = [20]
# ks = [1] + list(range(n // 10, n, n // 10)) + [n]
kofn_opponents = [deterministic_kofn_weights(k, n) for k in ks]
print(len(ks))
ks

print_every = 10
use_plus = True
mix_avg = 0.0
num_iterations = 1000

assert root_probs_tf is not None
assert transitions_tensor is not None

num_state_actions = num_states * num_actions

discount = tf.expand_dims(
    tf.where(
        tf.greater(sa_safety_info[0, :, -1], 0), tf.fill([num_states], 0.99),
        tf.zeros([num_states])), -1)

print(np.where(sasp_safety_info[:, -1, :, 0] > 0))

exit()


def fixed_reward_env(kofn_opponent):
    def env(policy):
        '''world X state'''
        v = dual_state_value_policy_evaluation_op(
            transitions_tensor, policy, reward_dataset, gamma=discount)
        '''state X action X world'''
        q = tf.transpose(
            reward_dataset +
            discount * tf.tensordot(v, transitions_tensor, axes=[-1, -1]),
            [1, 2, 0])
        v = tf.transpose(v)

        offset = 0
        kofn_q = []
        for sample_idx in range(num_samples_per_cfr_iter):
            next_offset = n * (sample_idx + 1)
            sample_v = v[:, offset:next_offset]
            sample_q = q[:, :, offset:next_offset]

            kofn_evs_and_weights = KofnEvsAndWeights(
                sample_v, kofn_opponent, context_weights=root_probs_tf)

            kofn_q.append(
                kofn_action_values(sample_q,
                                   kofn_evs_and_weights.world_weights))

            offset = next_offset
        return tf.reduce_mean(tf.stack(kofn_q, -1), axis=-1)

    return env


def fixed_reward_test_env(kofn_opponent):
    def env(policy):
        '''world X state'''
        v = dual_state_value_policy_evaluation_op(
            transitions_tensor, policy, test_reward_dataset, gamma=discount)
        v = tf.transpose(v)
        offset = 0
        kofn_ev = []
        for sample_idx in range(num_samples_per_cfr_iter):
            next_offset = n * (sample_idx + 1)

            kofn_ev.append(
                KofnEvsAndWeights(
                    v[:, offset:next_offset],
                    kofn_opponent,
                    context_weights=root_probs_tf).ev)

            offset = next_offset
        return tf.reduce_mean(tf.stack(kofn_ev, -1), axis=-1)

    return env


with tf.Session() as sess:

    print('Discounts: {}'.format(str(sess.run(discount))))

    @load_list
    def load_learners():
        learners = [
            UncertainRewardDiscountedContinuingKofnTabularCfr.load(
                'learner.{}'.format(i)) for i in range(len(ks))
        ]
        for learner in learners:
            sess.run(learner.cfr.policy_sum.initializer)
        return learners

    def save_learners(learners):
        for i, learner in enumerate(learners):
            learner.graph_save('learner.{}'.format(i), sess)

    def load_eot():
        return np.load('eot.npy')

    def save_eot(eot):
        np.save('eot', eot)

    def load_ci():
        return np.load('ci.npy')

    def save_ci(ci):
        np.save('ci', ci)

    def load_training_data():
        return load_learners(), load_eot(), load_ci()

    def save_training_data(learners_eot_ci):
        save_learners(learners_eot_ci[0])
        save_eot(learners_eot_ci[1])
        save_ci(learners_eot_ci[2])

    def compute_learners_and_eot():
        eot = []
        checkpoint_iterations = []

        reset_random(42)

        timer = Timer('create learners')

        with timer:
            learners = [
                KofnCfrLearner(
                    UncertainRewardDiscountedContinuingKofnTabularCfr.
                    from_num_states_and_actions(
                        num_states,
                        num_actions,
                        opponent=opponent,
                        use_plus=use_plus,
                        mix_avg=mix_avg,
                        next_policy_sum_code=0.9
                        #                 (
                        #                     UncertainRewardDiscountedContinuingKofnTabularCfr.USE_LINEAR_AVG
                        #                 )
                    ),
                    fixed_reward_env,
                    fixed_reward_test_env) for opponent in kofn_opponents
            ]
        print(timer)

        timer = Timer('create EV nodes')
        with timer:
            evs = [learner.test_ev() for learner in learners]
        print(timer)

        timer = Timer('create update nodes')
        with timer:
            updates = [learner.update()[-1] for learner in learners]
        print(timer)

        timer = Timer('init')
        with timer:
            sess.run(tf.global_variables_initializer())
        print(timer)

        timer = AccumulatingTimer('training')

        for t in range(1, num_iterations + 1):
            with timer:
                sess.run(updates)
                if t == 1 or t % print_every == 0:
                    evs_np = sess.run(evs)
                    eot.append(evs_np)
                    checkpoint_iterations.append(t)
                    print('{}: {}'.format(t, evs_np))
                    print('{}: {}'.format(t, str(timer)))
        return [l.cfr for l in learners], np.array(eot), checkpoint_iterations

    @load_or_save(load_training_data, save_training_data)
    def learners_and_eot():
        return compute_learners_and_eot()

    learners, eot_np, checkpoint_iterations = learners_and_eot()

    os.system('ls')

    colors = tableu20_color_table()
    colors = [next(colors) for _ in learners]

    line_styles = line_style_table()
    line_styles = [next(line_styles) for _ in learners]

    fig, axes_list = plt.subplots(1, 1)
    fig.set_size_inches((10, 5))
    for ki, k in enumerate(ks):
        axes_list.plot(
            checkpoint_iterations,
            eot_np[:, ki],
            label='{}-of-{}'.format(k, n),
            color=colors[ki],
            ls=line_styles[ki])
    axes_list.set_xlabel('iteration')
    axes_list.set_ylabel('EV')
    axes_list.set_ylim([-0.1, 2.5])
    plt.legend()
    plt.savefig('convergence.pdf')
    None

    timer = Timer('safety info')
    with timer:
        kofn_safety_info = np.array(
            sess.run([
                tf.reduce_sum(
                    (tf.expand_dims(root_probs_tf, axis=0) *
                     (1.0 - tf.transpose(discount)) *
                     dual_state_value_policy_evaluation_op(
                         transitions_tensor,
                         learner.policy(),
                         sa_safety_info,
                         gamma=discount)),
                    axis=-1) for learner in learners
            ]))
    print(timer)
print(kofn_safety_info.shape)
kofn_safety_info

(kofn_wall_collisions, kofn_pedestrians, kofn_bumps, kofn_ditch, kofn_speeds,
 kofn_progress, kofn_lane_changes) = [
     kofn_safety_info[:, j] for j in range(kofn_safety_info.shape[1])
 ]
kofn_bumps = kofn_bumps / 0.5
assert np.all(kofn_wall_collisions == 0)
assert np.all(kofn_pedestrians < 1e-5)
assert 0 <= max(kofn_bumps) <= 1 + 1e-5
assert 0 <= max(kofn_ditch) <= 1 + 1e-5
kofn_unsafe = (kofn_pedestrians + kofn_bumps + kofn_ditch) / 2.0
assert max(kofn_unsafe) <= 1 + 1e-5

colors = tableu20_color_table()
colors = [next(colors) for _ in ['random', 'k-of-n']]

line_styles = line_style_table()
line_styles = [next(line_styles) for _ in ['random', 'k-of-n']]

# plt.plot(
#     np.arange(num_worlds)/num_worlds,
#     sorted(speeds),
#     color=colors[0],
#     label='random',
#     marker='*',
#     ls='')
plt.plot(
    np.array(ks) / float(n),
    sorted(kofn_speeds),
    color=colors[1],
    label='k-of-n',
    marker='*',
    ls='')
plt.xlabel('fraction of policies or k/n')
plt.ylabel("avg speed")
plt.legend()
plt.ylim([0, 4])
plt.xlim([0, 1])
plt.savefig('speed.pdf')
None

# plt.plot(
#     np.arange(num_worlds) / num_worlds,
#     sorted(unsafe),
#     color=colors[0],
#     label='random',
#     marker='*',
#     ls=''
# )
plt.plot(
    np.array(ks) / float(n),
    sorted(kofn_unsafe),
    color=colors[1],
    label='k-of-n',
    marker='*',
    ls='')
plt.xlabel('fraction of policies or k/n')
plt.ylabel("avg time unsafe")
plt.legend()
plt.ylim([0, 0.5])
plt.xlim([0, 1])
plt.savefig('safety.pdf')
None
